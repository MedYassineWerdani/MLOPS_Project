{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5baf3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7696ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = './checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32876057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model = \"FacebookAI/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(language_model)\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9a19338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10832 entries, 0 to 10831\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   url        10832 non-null  object\n",
      " 1   topic      10832 non-null  object\n",
      " 2   date       10832 non-null  object\n",
      " 3   title      10832 non-null  object\n",
      " 4   site       10832 non-null  object\n",
      " 5   bias       10832 non-null  object\n",
      " 6   page_text  10832 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 592.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/data.tsv', sep='\\t')\n",
    "data.head()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9fa794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10832 rows\n",
      "Label distribution:\n",
      "bias\n",
      "right            2986\n",
      "leaning-left     2951\n",
      "left             1930\n",
      "leaning-right    1487\n",
      "center           1478\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Preprocessing text...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "# Use a lighter stopword set (remove stopwords but keep potentially meaningful ones)\n",
    "all_stopwords = set(stopwords.words('english'))\n",
    "# Keep words that might indicate bias\n",
    "keep_words = {'government', 'business', 'country', 'people', 'president', 'trump', 'biden', 'democrat', 'republican', 'left', 'right', 'liberal', 'conservative'}\n",
    "stop_words = all_stopwords - keep_words\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data/data.tsv', usecols=['page_text', 'bias'] , sep='\\t')\n",
    "df = df.dropna(subset=['page_text', 'bias']).reset_index(drop=True)\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "print(f\"Label distribution:\\n{df['bias'].value_counts()}\")\n",
    "\n",
    "# TEXT PREPROCESSING\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Comprehensive text preprocessing pipeline\"\"\"\n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags and entities\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'&[a-z]+;', '', text)\n",
    "    \n",
    "    # Remove special characters and digits (keep letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove short words (< 3 chars) - but less aggressively\n",
    "    words = text.split()\n",
    "    words = [w for w in words if len(w) >= 2]  # Changed from >= 3 to >= 2\n",
    "    \n",
    "    # Remove stopwords (lighter set)\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    \n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"\\nPreprocessing text...\")\n",
    "df['text'] = df['page_text'].apply(preprocess_text)\n",
    "\n",
    "# Remove rows with empty text after preprocessing\n",
    "df = df[df['text'].str.len() > 0].reset_index(drop=True)\n",
    "print(f\"After preprocessing: {len(df)} rows\")\n",
    "\n",
    "# Encode labels\n",
    "df['bias'] = pd.Categorical(df['bias'])\n",
    "df['label'] = df['bias'].cat.codes\n",
    "label_mapping_5 = dict(enumerate(df['bias'].cat.categories))\n",
    "print(f\"\\nOriginal 5-class mapping: {label_mapping_5}\")\n",
    "\n",
    "# Consolidate to 3 classes: Left, Center, Right\n",
    "def map_to_3_classes(label_code):\n",
    "    \"\"\"Map 5-class labels to 3-class labels\"\"\"\n",
    "    original_label = label_mapping_5[label_code].lower()\n",
    "    if 'left' in original_label:\n",
    "        return 0  # Left\n",
    "    elif 'right' in original_label:\n",
    "        return 2  # Right\n",
    "    else:  # center, least, etc.\n",
    "        return 1  # Center\n",
    "\n",
    "df['label'] = df['label'].map(map_to_3_classes)\n",
    "label_mapping = {0: 'Left', 1: 'Center', 2: 'Right'}\n",
    "print(f\"\\nConsolidated 3-class mapping: {label_mapping}\")\n",
    "print(f\"Number of classes: {df['label'].nunique()}\")\n",
    "print(f\"Samples per class:\\n{df['label'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb0596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerClassifier(\n",
      "  (transformer): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifer): Sequential(\n",
      "    (0): Dropout(p=0.1, inplace=False)\n",
      "    (1): Linear(in_features=768, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "class PandasTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df['page_text'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        return {'text': self.texts[idx], 'label': torch.tensor(self.labels[idx], dtype=torch.long)}\n",
    "\n",
    "dataset = PandasTextDataset(df)\n",
    "\n",
    "# Define model with 5 classes - added dropout for regularization\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, model_name, n_classes):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        layer_size = self.transformer.config.hidden_size\n",
    "\n",
    "        self.classifer = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(layer_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            x = self.transformer(input_ids=x, attention_mask=attention_mask)\n",
    "        x = x.last_hidden_state[:, 0, :]\n",
    "        x = self.classifer(x)\n",
    "        return x\n",
    "\n",
    "model = TransformerClassifier(language_model, 3).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c84019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset: 10832 samples\n",
      "Train: 7582 samples (237 batches)\n",
      "Validation: 1624 samples (51 batches)\n",
      "Test: 1626 samples (51 batches)\n",
      "Sample text length: 6498 chars\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "batch_size = 32\n",
    "n = len(dataset)\n",
    "train_len = int(n * 0.7)\n",
    "val_len = int(n * 0.15)\n",
    "test_len = n - train_len - val_len\n",
    "\n",
    "train_data, validation_data, test_data = torch.utils.data.random_split(dataset, [train_len, val_len, test_len])\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Total dataset: {n} samples\")\n",
    "print(f\"Train: {len(train_data)} samples ({len(train_loader)} batches)\")\n",
    "print(f\"Validation: {len(validation_data)} samples ({len(validation_loader)} batches)\")\n",
    "print(f\"Test: {len(test_data)} samples ({len(test_loader)} batches)\")\n",
    "for i in train_loader:\n",
    "    print(f\"Sample text length: {len(i['text'][0])} chars\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf0876",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Lower learning rate for better convergence on different models\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01, eps=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "\n",
    "def tokenize(text, device):\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return tokens['input_ids'].to(device), tokens['attention_mask'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ce47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|‚ñà‚ñà‚ñà‚ñç      | 82/237 [01:02<01:58,  1.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     63\u001b[39m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBEST ACC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mtrain_data\u001b[39m\u001b[34m(start_epoch, max_epochs, save_snapshots)\u001b[39m\n\u001b[32m     27\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     28\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# VALIDATE\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train_data(start_epoch=0 , max_epochs=2 , save_snapshots=True):\n",
    "    start_epoch = start_epoch\n",
    "    max_epochs = max_epochs  # More epochs for new models to converge\n",
    "    save_snapshots = True\n",
    "\n",
    "    if start_epoch != 0:\n",
    "        model.load_state_dict(torch.load(f\"{checkpoint_folder}/epoch-{start_epoch}.pth\"))\n",
    "\n",
    "    best_acc = 0\n",
    "    patience = 0\n",
    "    patience_limit = 4\n",
    "\n",
    "    for t in range(start_epoch+1, max_epochs+1):\n",
    "        print(f\"\\nepoch {t}: \", end='')\n",
    "\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for row in tqdm(train_loader):\n",
    "            tokens, attention_mask = tokenize(row['text'], device)\n",
    "            label = row[\"label\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(tokens, attention_mask)\n",
    "            loss = loss_fn(pred, label)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        print(f\"train_loss: {train_loss / len(train_loader):.4f}, \", end='')\n",
    "\n",
    "        # VALIDATE\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        print(f\"validation: \", end='')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for row in tqdm(validation_loader):\n",
    "                tokens, attention_mask = tokenize(row['text'], device)\n",
    "                label = row[\"label\"].to(device)\n",
    "                pred = model(tokens, attention_mask)\n",
    "                correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "                total_loss += loss_fn(pred, label).item()\n",
    "\n",
    "        avg_error = total_loss / len(validation_loader)\n",
    "        accuracy = correct / len(validation_loader.dataset)\n",
    "        print(\"error: {:.4f}, accuracy: {:.4f}\".format(avg_error, accuracy))\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(accuracy)\n",
    "\n",
    "        if save_snapshots and accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            torch.save(model.state_dict(),  f\"{checkpoint_folder}/epoch-{t}.pth\")\n",
    "            print(f\"  ‚Üí Saved checkpoint! Best so far: {best_acc:.4f}\")\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= patience_limit:\n",
    "                print(f\"  ‚Üí Early stopping triggered (no improvement for {patience_limit} epochs)\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nBEST ACC: {best_acc:.4f}\")\n",
    "\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f3314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141/141 [01:40<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.37048758598084147\n",
      "accuracy: 0.9913333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "def test(filename):\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for row in tqdm(test_loader):\n",
    "            tokens, attention_mask = tokenize(row['text'], device)\n",
    "            label = row[\"label\"].to(device)\n",
    "            pred = model(tokens, attention_mask)\n",
    "            correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "            total_loss += loss_fn(pred, label).item()\n",
    "\n",
    "    avg_error = total_loss / len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(\"error:\", avg_error)\n",
    "    print(\"accuracy:\", accuracy)\n",
    "test(f\"{checkpoint_folder}/epoch-2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f7e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f14d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL COMPARISON: DeBERTa vs BERT\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "models_to_train = [\n",
    "    \"microsoft/deberta-base\",\n",
    "    \"bert-base-uncased\"\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "def train_model(model_name, max_epochs=5):\n",
    "    \"\"\"Train a model and return metrics\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer_new = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_new = TransformerClassifier(model_name, 3).to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer_new = torch.optim.AdamW(model_new.parameters(), lr=5e-5, weight_decay=0.01, eps=1e-8)\n",
    "    scheduler_new = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_new, mode='max', factor=0.5, patience=2)\n",
    "    \n",
    "    # Store metrics\n",
    "    metrics = {\n",
    "        'model_name': model_name,\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'val_accuracies': [],\n",
    "        'epochs_trained': 0,\n",
    "        'best_accuracy': 0,\n",
    "        'start_time': time.time(),\n",
    "        'training_time': 0\n",
    "    }\n",
    "    \n",
    "    def tokenize_new(text):\n",
    "        tokens = tokenizer_new(\n",
    "            text,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        return tokens['input_ids'].to(device), tokens['attention_mask'].to(device)\n",
    "    \n",
    "    best_acc = 0\n",
    "    patience = 0\n",
    "    patience_limit = 4\n",
    "    \n",
    "    for t in range(1, max_epochs+1):\n",
    "        print(f\"Epoch {t}/{max_epochs}: \", end='')\n",
    "        \n",
    "        # TRAIN\n",
    "        model_new.train()\n",
    "        train_loss = 0\n",
    "        for row in tqdm(train_loader, disable=True):\n",
    "            tokens, attention_mask = tokenize_new(row['text'])\n",
    "            label = row[\"label\"].to(device)\n",
    "            \n",
    "            optimizer_new.zero_grad()\n",
    "            pred = model_new(tokens, attention_mask)\n",
    "            loss = loss_fn(pred, label)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model_new.parameters(), 1.0)\n",
    "            optimizer_new.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        metrics['train_losses'].append(avg_train_loss)\n",
    "        print(f\"train_loss: {avg_train_loss:.4f} | \", end='')\n",
    "        \n",
    "        # VALIDATE\n",
    "        model_new.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for row in tqdm(validation_loader, disable=True):\n",
    "                tokens, attention_mask = tokenize_new(row['text'])\n",
    "                label = row[\"label\"].to(device)\n",
    "                pred = model_new(tokens, attention_mask)\n",
    "                correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "                total_loss += loss_fn(pred, label).item()\n",
    "        \n",
    "        avg_val_loss = total_loss / len(validation_loader)\n",
    "        accuracy = correct / len(validation_loader.dataset)\n",
    "        \n",
    "        metrics['val_losses'].append(avg_val_loss)\n",
    "        metrics['val_accuracies'].append(accuracy)\n",
    "        \n",
    "        print(f\"val_loss: {avg_val_loss:.4f} | val_acc: {accuracy:.4f}\")\n",
    "        \n",
    "        scheduler_new.step(accuracy)\n",
    "        \n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            metrics['best_accuracy'] = best_acc\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= patience_limit:\n",
    "                print(f\"Early stopping triggered (no improvement for {patience_limit} epochs)\")\n",
    "                break\n",
    "        \n",
    "        metrics['epochs_trained'] = t\n",
    "    \n",
    "    metrics['training_time'] = time.time() - metrics['start_time']\n",
    "    \n",
    "    # TEST\n",
    "    print(f\"\\nTesting {model_name}...\")\n",
    "    model_new.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for row in tqdm(test_loader, disable=True):\n",
    "            tokens, attention_mask = tokenize_new(row['text'])\n",
    "            label = row[\"label\"].to(device)\n",
    "            pred = model_new(tokens, attention_mask)\n",
    "            correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "            total_loss += loss_fn(pred, label).item()\n",
    "    \n",
    "    test_loss = total_loss / len(test_loader)\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    metrics['test_loss'] = test_loss\n",
    "    metrics['test_accuracy'] = test_accuracy\n",
    "    \n",
    "    # Model size\n",
    "    param_count = sum(p.numel() for p in model_new.parameters())\n",
    "    metrics['parameters'] = param_count\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Train both models\n",
    "for model_name in models_to_train:\n",
    "    results[model_name] = train_model(model_name, max_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa7ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON METRICS\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for model_name, metrics in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name.split('/')[-1],\n",
    "        'Test Accuracy': f\"{metrics['test_accuracy']:.4f}\",\n",
    "        'Test Loss': f\"{metrics['test_loss']:.4f}\",\n",
    "        'Best Val Accuracy': f\"{metrics['best_accuracy']:.4f}\",\n",
    "        'Epochs Trained': metrics['epochs_trained'],\n",
    "        'Training Time (s)': f\"{metrics['training_time']:.2f}\",\n",
    "        'Parameters': f\"{metrics['parameters']:,}\",\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Detailed metrics\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Test Accuracy:        {metrics['test_accuracy']:.4f}\")\n",
    "    print(f\"Test Loss:            {metrics['test_loss']:.4f}\")\n",
    "    print(f\"Best Val Accuracy:    {metrics['best_accuracy']:.4f}\")\n",
    "    print(f\"Epochs Trained:       {metrics['epochs_trained']}\")\n",
    "    print(f\"Training Time:        {metrics['training_time']:.2f} seconds ({metrics['training_time']/60:.2f} minutes)\")\n",
    "    print(f\"Parameters:           {metrics['parameters']:,}\")\n",
    "    print(f\"Avg Params/Epoch:     {metrics['parameters']/metrics['epochs_trained']:,.0f}\")\n",
    "    print(f\"Final Train Loss:     {metrics['train_losses'][-1]:.4f}\")\n",
    "    print(f\"Final Val Loss:       {metrics['val_losses'][-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e107ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION: Model Comparison Plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Model Comparison: DeBERTa vs BERT', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e']\n",
    "\n",
    "# 1. Validation Accuracy Over Epochs\n",
    "ax = axes[0, 0]\n",
    "for idx, (model_name, metrics) in enumerate(results.items()):\n",
    "    epochs = range(1, len(metrics['val_accuracies']) + 1)\n",
    "    ax.plot(epochs, metrics['val_accuracies'], marker='o', label=model_name.split('/')[-1], color=colors[idx], linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Validation Accuracy')\n",
    "ax.set_title('Validation Accuracy Progression')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Validation Loss Over Epochs\n",
    "ax = axes[0, 1]\n",
    "for idx, (model_name, metrics) in enumerate(results.items()):\n",
    "    epochs = range(1, len(metrics['val_losses']) + 1)\n",
    "    ax.plot(epochs, metrics['val_losses'], marker='s', label=model_name.split('/')[-1], color=colors[idx], linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Validation Loss')\n",
    "ax.set_title('Validation Loss Progression')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Test Accuracy Comparison\n",
    "ax = axes[1, 0]\n",
    "model_labels = [m.split('/')[-1] for m in results.keys()]\n",
    "test_accuracies = [results[m]['test_accuracy'] for m in results.keys()]\n",
    "bars = ax.bar(model_labels, test_accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_title('Test Accuracy Comparison')\n",
    "ax.set_ylim([0, 1])\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{test_accuracies[i]:.4f}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Training Time vs Parameters\n",
    "ax = axes[1, 1]\n",
    "for idx, (model_name, metrics) in enumerate(results.items()):\n",
    "    training_time = metrics['training_time']\n",
    "    params = metrics['parameters'] / 1e6  # Convert to millions\n",
    "    ax.scatter(params, training_time, s=300, color=colors[idx], \n",
    "               label=model_name.split('/')[-1], edgecolor='black', linewidth=2)\n",
    "    ax.annotate(model_name.split('/')[-1], (params, training_time),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "ax.set_xlabel('Parameters (Millions)')\n",
    "ax.set_ylabel('Training Time (seconds)')\n",
    "ax.set_title('Training Efficiency')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eabfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL RECOMMENDATION & ANALYSIS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS & RECOMMENDATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Find best performer\n",
    "best_model = max(results.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "worst_model = min(results.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "\n",
    "print(f\"üèÜ Best Performing Model: {best_model[0]}\")\n",
    "print(f\"   Test Accuracy: {best_model[1]['test_accuracy']:.4f}\")\n",
    "print(f\"   Test Loss: {best_model[1]['test_loss']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Fastest Model: \", end=\"\")\n",
    "fastest = min(results.items(), key=lambda x: x[1]['training_time'])\n",
    "print(f\"{fastest[0]}\")\n",
    "print(f\"   Training Time: {fastest[1]['training_time']:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nüìä Most Parameters: \", end=\"\")\n",
    "largest = max(results.items(), key=lambda x: x[1]['parameters'])\n",
    "print(f\"{largest[0]}\")\n",
    "print(f\"   Parameters: {largest[1]['parameters']:,}\")\n",
    "\n",
    "# Efficiency score (accuracy per second per million params)\n",
    "print(f\"\\n‚ö° Efficiency Score (Test Acc per Sec per M Params):\")\n",
    "for model_name, metrics in results.items():\n",
    "    efficiency = metrics['test_accuracy'] / (metrics['training_time'] * (metrics['parameters'] / 1e6))\n",
    "    print(f\"   {model_name.split('/')[-1]}: {efficiency:.6f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n{'Model':<20} {'Accuracy':<12} {'Speed':<12} {'Parameters':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for model_name, metrics in results.items():\n",
    "    model_short = model_name.split('/')[-1]\n",
    "    print(f\"{model_short:<20} {metrics['test_accuracy']:<12.4f} {metrics['training_time']:<12.1f}s {metrics['parameters']:<15,}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RECOMMENDATION FOR POLITICAL BIAS DETECTION\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Compare accuracy difference\n",
    "accuracy_values = [m['test_accuracy'] for m in results.values()]\n",
    "best_acc = max(accuracy_values)\n",
    "acc_diff = best_acc - min(accuracy_values)\n",
    "\n",
    "if best_model[0] == \"microsoft/deberta-base\":\n",
    "    print(f\"\\n‚úì DeBERTa-base shows superior performance for political bias detection\")\n",
    "    print(f\"  ‚Ä¢ {acc_diff*100:.2f}% accuracy improvement over BERT\")\n",
    "    print(f\"  ‚Ä¢ Better at capturing subtle linguistic patterns in political text\")\n",
    "    print(f\"  ‚Ä¢ Improved attention mechanisms for nuanced bias detection\")\n",
    "    print(f\"\\n‚Üí RECOMMENDED: Use DeBERTa-base for production deployment\")\n",
    "else:\n",
    "    print(f\"\\n‚úì BERT-base-uncased is sufficient for this task\")\n",
    "    print(f\"  ‚Ä¢ Comparable accuracy to DeBERTa ({best_model[1]['test_accuracy']:.4f})\")\n",
    "    print(f\"  ‚Ä¢ Faster training time ({fastest[1]['training_time']:.1f}s)\")\n",
    "    print(f\"  ‚Ä¢ More widely supported and stable\")\n",
    "    print(f\"\\n‚Üí RECOMMENDED: Use BERT-base-uncased for speed, or DeBERTa for accuracy\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
