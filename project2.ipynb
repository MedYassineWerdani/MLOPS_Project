{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5baf3d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\.conda\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323eff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = './checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32876057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model = \"FacebookAI/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(language_model)\n",
    "device = \"cpu\"\n",
    "# GPU can cause memory issues - using CPU for stability\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9fa794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load local TSV (expects columns 'text' and 'label')\n",
    "import os\n",
    "df = pd.read_csv(\"data/data.tsv\", sep='\\t')\n",
    "if 'text' not in df.columns or 'label' not in df.columns:\n",
    "    raise ValueError(\"data.tsv must contain 'text' and 'label' columns\")\n",
    "# If labels are strings, convert to integer categories\n",
    "if df['label'].dtype == object:\n",
    "    df['label'] = df['label'].astype('category').cat.codes\n",
    "class LocalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self._df = df.reset_index(drop=True)\n",
    "    def to_pandas(self):\n",
    "        return self._df\n",
    "    def __len__(self):\n",
    "        return len(self._df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self._df.iloc[idx]\n",
    "        return {'text': row['text'], 'label': int(row['label'])}\n",
    "dataset = {'train': LocalDataset(df)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9fb0596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerClassifier(\n",
      "  (transformer): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifer): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (1): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, model_name, n_classes):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        layer_size = self.transformer.config.hidden_size\n",
    "\n",
    "        self.classifer = nn.Sequential(\n",
    "            nn.Linear(layer_size, n_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            x = self.transformer(input_ids=x, attention_mask=attention_mask)\n",
    "        x = x.last_hidden_state[:, 0, :]\n",
    "        x = self.classifer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = TransformerClassifier(language_model, 2).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73c84019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President Raul Castro called on Cubans on Monday to unite in swiftly rebuilding the Caribbean nation in the wake of Hurricane Irma, which killed at least 10 people during a devastating three-day rampage along the length of the island. The storm crashed into Cuba late on Friday, with sustained winds of than 157 miles per hour (253 km per hour). It tore along the island s northern shore for some 200 miles (322 km) - lashing tourist resorts on the island s pristine keys - before turning northward to batter Florida. In Havana, people set about removing debris from the streets on Monday and mopping up homes hit by widespread flooding. The hurricane - the first Category 5 storm to make landfall in Cuba since 1932 - tore off roofs, felled trees and downed electricity poles, leaving millions without power and water. State media said on Monday Irma had seriously damaged Cuba s already dilapidated sugar industry, flooding and flattening an extensive area of sugar cane.  Given the immensity of its size, practically no region escaped its impact,  Castro said in a statement published in state-run media, urging Cubans to unite to rebuild the country.  The task we have before us is immense but, with a people like ours, we will win the most important battle: the recovery.  Castro, 86, who has said he will step aside early next year, said authorities had not been able to assess the full extent of damage yet, but the hurricane had impacted housing stock and the power grid, as well as agriculture. The fatalities in Cuba brought the death toll from Irma to 39 in the Caribbean. The hurricane dealt a potentially heavy blow to Cuba s agriculture- and tourism-reliant economy at a difficult moment, with an economic reform program appearing stalled and aid from key ally Venezuela shrinking. Seven of the Cuban dead were in the province of Havana, which only caught the outer reaches of the hurricane but still saw waves of up to 36 feet (12 meters) pummel its historic seafront boulevard on Sunday. The two youngest victims, Mar a del Carmen Arregoit a and  Yolendis Castillo, both 27, died when a balcony crashed down onto their bus in central Havana, infamous for its creaking infrastructure. The oldest, Nieves Mart nez, 89, was found floating in water in front of her home in the Vedado district of Havana in the wake of heavy flooding in the capital, according to a statement from civil defense authorities. Large parts of Havana remained underwater on Monday.  We have absolutely nothing, we lost it all, the fridge, the washing machine, we lost it all here,  said Ayda Herrera, whose home on Havana s seafront boulevard was flooded. Fatalities also were reported in Matanzas, home to the tourist resort of Varadero, and the regions of Ciego de Avila and Camaguey farther east. While many of Cuba s top resorts on the northern islands took a direct hit from the hurricane, Castro said they would be fixed up in time for high tourist season at the end of the year. Many tourists flew home ahead of Irma, but others hunkered down in hotels and shelters.  Tropical paradise turned into hell for us,  said Spanish tourist Michel Munoz, 31, who said he spent the hurricane sheltering in a Havana convent and had struggled to find food on Sunday as most restaurants were closed. Excavators have started the laborious task of removing debris and fallen trees from roads throughout the island and trucks carried drinkable water to the worst-affected areas.  We are working twice as hard as usual to ensure as much bread as possible to the population,  said Alain Alfonso, a worker at a bakery in Central Havana.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n = len(dataset['train'])\n",
    "train_data, validation_data, test_data= torch.utils.data.random_split(dataset['train'], [int(n * 0.7), int(n * 0.15), int(n * 0.15)])\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for i in train_loader:\n",
    "    print(i['text'][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9439ff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in train:\n",
      "label\n",
      "0    15478\n",
      "1    14522\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check Hugging Face dataset structure\n",
    "print(\"\\nLabel distribution in train:\")\n",
    "print(dataset['train'].to_pandas()['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37bf0876",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def tokenize(text, device):\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    return tokens['input_ids'].to(device), tokens['attention_mask'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ce47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 1/2 - roberta-base\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 10/657 [00:08<08:14,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10: Loss = 0.6902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 20/657 [00:15<07:52,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 20: Loss = 0.6823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 30/657 [00:22<07:44,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 30: Loss = 0.6761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 36/657 [00:27<07:40,  1.35it/s]"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "max_epochs = 2\n",
    "save_snapshots = True\n",
    "\n",
    "if start_epoch != 0:\n",
    "    model.load_state_dict(torch.load(f\"{checkpoint_folder}/epoch-{start_epoch}.pth\"))\n",
    "\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "for t in range(start_epoch+1, max_epochs+1):\n",
    "    print(f\"epoch {t}: \", end='')\n",
    "\n",
    "\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    for row in tqdm(train_loader):\n",
    "        tokens, attention_mask = tokenize(row['text'], device)\n",
    "        label = row[\"label\"].to(device)\n",
    "\n",
    "        loss_fn(model(tokens, attention_mask), label).backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # VALIDATE\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    print(f\"validation: \", end='')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for row in tqdm(validation_loader):\n",
    "            tokens, attention_mask = tokenize(row['text'], device)\n",
    "            label = row[\"label\"].to(device)\n",
    "            pred = model(tokens, attention_mask)\n",
    "            correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "            total_loss += loss_fn(pred, label).item()\n",
    "\n",
    "    avg_error = total_loss / len(validation_loader)\n",
    "    accuracy = correct / len(validation_loader.dataset)\n",
    "    print(\"error:\", avg_error)\n",
    "    print(\"accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "    if save_snapshots and accuracy > best_acc:\n",
    "        best_acc = accuracy\n",
    "        torch.save(model.state_dict(),  f\"{checkpoint_folder}/epoch-{t}.pth\")\n",
    "print(\"BEST ACC:\", best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f3314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [05:15<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.3429108549517097\n",
      "accuracy: 0.9895555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "def test(filename):\n",
    "    model.load_state_dict(torch.load(filename, weights_only=True))\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for row in tqdm(test_loader):\n",
    "            tokens, attention_mask = tokenize(row['text'], device)\n",
    "            label = row[\"label\"].to(device)\n",
    "            pred = model(tokens, attention_mask)\n",
    "            correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "            total_loss += loss_fn(pred, label).item()\n",
    "\n",
    "    avg_error = total_loss / len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(\"error:\", avg_error)\n",
    "    print(\"accuracy:\", accuracy)\n",
    "test(f\"{checkpoint_folder}\\epoch-4.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c599b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
